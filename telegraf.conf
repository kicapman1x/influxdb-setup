[global_tags]
  environment = "dev"
  tenant= "daddy"

[agent]
  #how often collect /input
  interval = "30s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  #no jitter, i testing only - input 
  collection_jitter = "0s"
  #how often flush /output
  flush_interval = "30s"
  #output
  flush_jitter = "0s"
  #same as collection rounded then
  precision = "0s"
  #if u feel insecure
  # debug = true
  #if u feel confident 
  # quiet = true

  logformat = "text"
  #probs important if want to ingest logs to elastic
  # structured_log_message_key = "message"
  logfile = "${INFLUX_HOME}/logs/telegraf-daddy.log"
  logfile_rotation_interval = "1d"
  logfile_rotation_max_size = "10MB"
  logfile_rotation_max_archives = 3
  #dont step one pai angmor
  log_with_timezone = "local"

  hostname = "callmiidaddy"
  omit_hostname = false
  #this one for instrumenting infra metrics? not sure 
  snmp_translator = "gosmi"

  statefile = "${INFLUX_HOME}/data/telegraf-daddy-statefile.db"

  skip_processors_after_aggregators = false

###############################################################################
#                            SECRETSTORE PLUGINS                              #
###############################################################################
#If you have any secretstores, my pw in env var since I only user lol

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


# # Configuration for sending metrics to InfluxDB 2.0
[[outputs.influxdb_v2]]
  urls = ["https://influx.han.gg:8086"]
#   ## Token for authentication.
  token = "${INFDB_ADMIN_TOKEN}"
#   ## Organization is the name of the organization you wish to write to.
  organization = "admin_org"
#   ## Destination bucket to write into.
  bucket = "admin_bucket"

  insecure_skip_verify = true

# [[outputs.discard]]
#   # lmao u trolling ah

#Keep this in case need to debug I guess
# # Send telegraf metrics to file(s)
# [[outputs.file]]
#   ## Files to write to, "stdout" is a specially handled file.
#   files = ["stdout", "/tmp/metrics.out"]
#
#   ## Use batch serialization format instead of line based delimiting.  The
#   ## batch format allows for the production of non line based output formats and
#   ## may more efficiently encode and write metrics.
#   # use_batch_format = false
#
#   ## The file will be rotated after the time interval specified.  When set
#   ## to 0 no time based rotation is performed.
#   # rotation_interval = "0h"
#
#   ## The logfile will be rotated when it becomes larger than the specified
#   ## size.  When set to 0 no size based rotation is performed.
#   # rotation_max_size = "0MB"
#
#   ## Maximum number of rotated archives to keep, any older logs are deleted.
#   ## If set to -1, no archives are removed.
#   # rotation_max_archives = 5
#
#   ## Data format to output.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"
#
#   ## Compress output data with the specified algorithm.
#   ## If empty, compression will be disabled and files will be plain text.
#   ## Supported algorithms are "zstd", "gzip" and "zlib".
#   # compression_algorithm = ""
#
#   ## Compression level for the algorithm above.
#   ## Please note that different algorithms support different levels:
#   ##   zstd  -- supports levels 1, 3, 7 and 11.
#   ##   gzip -- supports levels 0, 1 and 9.
#   ##   zlib -- supports levels 0, 1, and 9.
#   ## By default the default compression level for each algorithm is used.
#   # compression_level = -1

# # Configurable HTTP health check resource based on metrics
# [[outputs.health]]
#   ## Address and port to listen on.
#   ##   ex: service_address = "http://localhost:8080"
#   ##       service_address = "unix:///var/run/telegraf-health.sock"
#   # service_address = "http://:8080"
#
#   ## The maximum duration for reading the entire request.
#   # read_timeout = "5s"
#   ## The maximum duration for writing the entire response.
#   # write_timeout = "5s"
#
#   ## Username and password to accept for HTTP basic authentication.
#   # basic_username = "user1"
#   # basic_password = "secret"
#
#   ## Allowed CA certificates for client certificates.
#   # tls_allowed_cacerts = ["/etc/telegraf/clientca.pem"]
#
#   ## TLS server certificate and private key.
#   # tls_cert = "/etc/telegraf/cert.pem"
#   # tls_key = "/etc/telegraf/key.pem"
#
#   ## Maximum expected time between metrics being written
#   ## Enforces an unhealthy state if there was no new metric seen for at least
#   ## the specified time. The check is disabled by default and only used if a
#   ## positive time is specified.
#   # max_time_between_metrics = "0s"
#
#   ## NOTE: Due to the way TOML is parsed, tables must be at the END of the
#   ## plugin definition, otherwise additional config options are read as part of
#   ## the table
#
#   ## One or more check sub-tables should be defined, it is also recommended to
#   ## use metric filtering to limit the metrics that flow into this output.
#   ##
#   ## When using the default buffer sizes, this example will fail when the
#   ## metric buffer is half full.
#   ##
#   ## namepass = ["internal_write"]
#   ## tagpass = { output = ["influxdb"] }
#   ##
#   ## [[outputs.health.compares]]
#   ##   field = "buffer_size"
#   ##   lt = 5000.0
#   ##
#   ## [[outputs.health.contains]]
#   ##   field = "buffer_size"

###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################

###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

#OTEL81
[[inputs.opentelemetry]]
  grpc_endpoint = "0.0.0.0:4317"

# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states
  ## NOTE: The resulting 'time_active' field INCLUDES 'iowait'!
  report_active = false
  ## If true and the info is available then add core_id and physical_id tags
  core_tags = false


# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  # mount_points = ["/"]

  ## Ignore mount points by filesystem type.
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs"]

  ## Ignore mount points by mount options.
  ## The 'mount' command reports options of all mounts in parathesis.
  ## Bind mounts can be ignored with the special 'bind' option.
  # ignore_mount_opts = []


# Read metrics about disk IO by device
[[inputs.diskio]]
  ## Devices to collect stats for
  ## Wildcards are supported except for disk synonyms like '/dev/disk/by-id'.
  ## ex. devices = ["sda", "sdb", "vd*", "/dev/disk/by-id/nvme-eui.00123deadc0de123"]
  devices = ["*"]

  ## Skip gathering of the disk's serial numbers.
  skip_serial_number = true

  ## Device metadata tags to add on systems supporting it (Linux only)
  ## Use 'udevadm info -q property -n <device>' to get a list of properties.
  ## Note: Most, but not all, udev properties can be accessed this way. Properties
  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.
  device_tags = ["ID_FS_TYPE", "ID_FS_USAGE"]

# Plugin to collect various Linux kernel statistics.
# This plugin ONLY supports Linux
[[inputs.kernel]]
  ## Additional gather options
  ## Possible options include:
  ## * ksm - kernel same-page merging
  ## * psi - pressure stall information
  # collect = []

# Read metrics about memory usage
[[inputs.mem]]
  # no configuration

# Get the number of processes and group them by status
# This plugin ONLY supports non-Windows
[[inputs.processes]]
  ## Use sudo to run ps command on *BSD systems. Linux systems will read
  ## /proc, so this does not apply there.
  use_sudo = true

# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration

# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration

# # Returns ethtool statistics for given interfaces
# # This plugin ONLY supports Linux
[[inputs.ethtool]]
#   ## List of interfaces to pull metrics for
  interface_include = ["wlan0"]
  down_interfaces = "expose"

[[inputs.kafka_consumer]]
  brokers = ["kafka-mq.han.gg:9092"]
  topics  = ["device-config-ingest"]
  consumer_group = "telegraf_device_status"
  offset = "oldest"
  version = "4.0.0"
  data_format = "json_v2"
  
  msg_header_as_metric_name = "X-Metric-Name"
  timestamp_source = "inner"

  tls_ca   = "/home/daddy/apps/lab_infra/ssl/certs/ca.pem"
  tls_cert = "/home/daddy/apps/lab_infra/ssl/certs/han.gg.crt"
  tls_key  = "/home/daddy/apps/lab_infra/ssl/certs/han.gg.key"

  [[inputs.kafka_consumer.json_v2]]

    [[inputs.kafka_consumer.json_v2.tag]]
      path = "deviceId"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "deviceStatus"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "deviceType"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "deviceMode"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "location.site"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "location.harbour"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "location.area"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "location.direction"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "location.zone"
    [[inputs.kafka_consumer.json_v2.tag]]
      path = "eventTrigger"

    [[inputs.kafka_consumer.json_v2.field]]
      path = "indicators.0.AllBypassFlight"
      type = "string"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "indicators.1.SiloBypass"
      type = "string"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "indicators.2.CheckSystemEnabled"
      type = "string"

[[inputs.kafka_consumer]]
  brokers = ["kafka-mq.han.gg:9092"]
  topics  = ["device-metrics-ingest"]
  consumer_group = "telegraf_device_metrics"
  offset = "oldest"
  version = "4.0.0"

  data_format = "json_v2"
  msg_header_as_metric_name = "X-Metric-Name"
  timestamp_source = "inner"

  tls_ca   = "/home/daddy/apps/lab_infra/ssl/certs/ca.pem"
  tls_cert = "/home/daddy/apps/lab_infra/ssl/certs/han.gg.crt"
  tls_key  = "/home/daddy/apps/lab_infra/ssl/certs/han.gg.key"

  [[inputs.kafka_consumer.json_v2]]

    [[inputs.kafka_consumer.json_v2.tag]]
      path = "deviceid"

    [[inputs.kafka_consumer.json_v2.field]]
      path = "metrics.cpu_usage"
      type = "float"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "metrics.mem_usage"
      type = "float"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "metrics.throughput"
      type = "float"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "metrics.error_count"
      type = "float"
    [[inputs.kafka_consumer.json_v2.field]]
      path = "metrics.avg_clearance_time"
      type = "float"

#for testing lo
# # Parse a complete file each interval
# [[inputs.file]]
#   ## Files to parse each interval.  Accept standard unix glob matching rules,
#   ## as well as ** to match recursive files and directories.
#   files = ["/tmp/metrics.out"]
#
#   ## Character encoding to use when interpreting the file contents.  Invalid
#   ## characters are replaced using the unicode replacement character.  When set
#   ## to the empty string the data is not decoded to text.
#   ##   ex: character_encoding = "utf-8"
#   ##       character_encoding = "utf-16le"
#   ##       character_encoding = "utf-16be"
#   ##       character_encoding = ""
#   # character_encoding = ""
#
#   ## Data format to consume.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
#   data_format = "influx"
#
#   ## Please use caution when using the following options: when file name
#   ## variation is high, this can increase the cardinality significantly. Read
#   ## more about cardinality here:
#   ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality
#
#   ## Name of tag to store the name of the file. Disabled if not set.
#   # file_tag = ""
#
#   ## Name of tag to store the absolute path and name of the file. Disabled if
#   ## not set.
#   # file_path_tag = ""


[[inputs.internal]]
  collect_memstats = true
  collect_gostats = true
# # Monitors internet speed using speedtest.net service
[[inputs.internet_speed]]
  interval = "60m"
  connections = 0

[[inputs.wireless]]

[[inputs.kernel_vmstat]]
#   # no configuration

# # Provides Linux CPU metrics
# # This plugin ONLY supports Linux
# [[inputs.linux_cpu]]
#   ## Path for sysfs filesystem.
#   ## See https://www.kernel.org/doc/Documentation/filesystems/sysfs.txt
#   ## Defaults:
#   # host_sys = "/sys"
#
#   ## CPU metrics collected by the plugin.
#   ## Supported options:
#   ## "cpufreq", "thermal"
#   ## Defaults:
#   # metrics = ["cpufreq"]

# # Read metrics about LVM physical volumes, volume groups, logical volumes.
# [[inputs.lvm]]
#   ## Use sudo to run LVM commands
#   use_sudo = false
#
#   ## The default location of the pvs binary can be overridden with:
#   #pvs_binary = "/usr/sbin/pvs"
#
#   ## The default location of the vgs binary can be overridden with:
#   #vgs_binary = "/usr/sbin/vgs"
#
#   ## The default location of the lvs binary can be overridden with:
#   #lvs_binary = "/usr/sbin/lvs"

# # Gather metrics about network interfaces
# [[inputs.net]]
#   ## By default, telegraf gathers stats from any up interface (excluding loopback)
#   ## Setting interfaces will tell it to gather these explicit interfaces,
#   ## regardless of status. When specifying an interface, glob-style
#   ## patterns are also supported.
#   # interfaces = ["eth*", "enp0s[0-1]", "lo"]
#
#   ## On linux systems telegraf also collects protocol stats.
#   ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.
#   ##
#   ## DEPRECATION NOTICE: A value of 'false' is deprecated and discouraged!
#   ##                     Please set this to `true` and use the 'inputs.nstat'
#   ##                     plugin instead.
#   # ignore_protocol_stats = false

# # Collect response time of a TCP or UDP connection
# [[inputs.net_response]]
#   ## Protocol, must be "tcp" or "udp"
#   ## NOTE: because the "udp" protocol does not respond to requests, it requires
#   ## a send/expect string pair (see below).
#   protocol = "tcp"
#   ## Server address (default localhost)
#   address = "localhost:80"
#
#   ## Set timeout
#   # timeout = "1s"
#
#   ## Set read timeout (only used if expecting a response)
#   # read_timeout = "1s"
#
#   ## The following options are required for UDP checks. For TCP, they are
#   ## optional. The plugin will send the given string to the server and then
#   ## expect to receive the given 'expect' string back.
#   ## string sent to the server
#   # send = "ssh"
#   ## expected string in answer
#   # expect = "ssh"
#
#   ## Uncomment to remove deprecated fields; recommended for new deploys
#   # fieldexclude = ["result_type", "string_found"]


# # Read TCP metrics such as established, time wait and sockets counts.
# [[inputs.netstat]]
#   # no configuration

# # Derive metrics from aggregating OpenSearch query results
# [[inputs.opensearch_query]]
#   ## OpenSearch cluster endpoint(s). Multiple urls can be specified as part
#   ## of the same cluster.  Only one successful call will be made per interval.
#   urls = [ "https://node1.os.example.com:9200" ] # required.
#
#   ## OpenSearch client timeout, defaults to "5s".
#   # timeout = "5s"
#
#   ## HTTP basic authentication details
#   # username = "admin"
#   # password = "admin"
#
#   ## Skip TLS validation.  Useful for local testing and self-signed certs.
#   # insecure_skip_verify = false
#
#   [[inputs.opensearch_query.aggregation]]
#     ## measurement name for the results of the aggregation query
#     measurement_name = "measurement"
#
#     ## OpenSearch index or index pattern to search
#     index = "index-*"
#
#     ## The date/time field in the OpenSearch index (mandatory).
#     date_field = "@timestamp"
#
#     ## If the field used for the date/time field in OpenSearch is also using
#     ## a custom date/time format it may be required to provide the format to
#     ## correctly parse the field.
#     ##
#     ## If using one of the built in OpenSearch formats this is not required.
#     ## https://opensearch.org/docs/2.4/opensearch/supported-field-types/date/#built-in-formats
#     # date_field_custom_format = ""
#
#     ## Time window to query (eg. "1m" to query documents from last minute).
#     ## Normally should be set to same as collection interval
#     query_period = "1m"
#
#     ## Lucene query to filter results
#     # filter_query = "*"
#
#     ## Fields to aggregate values (must be numeric fields)
#     # metric_fields = ["metric"]
#
#     ## Aggregation function to use on the metric fields
#     ## Must be set if 'metric_fields' is set
#     ## Valid values are: avg, sum, min, max, sum
#     # metric_function = "avg"
#
#     ## Fields to be used as tags.  Must be text, non-analyzed fields. Metric
#     ## aggregations are performed per tag
#     # tags = ["field.keyword", "field2.keyword"]
#
#     ## Set to true to not ignore documents when the tag(s) above are missing
#     # include_missing_tag = false
#
#     ## String value of the tag when the tag does not exist
#     ## Required when include_missing_tag is true
#     # missing_tag_value = "null"

# # Monitor process cpu and memory usage
# [[inputs.procstat]]
#   ## PID file to monitor process
#   pid_file = "/var/run/nginx.pid"
#   ## executable name (ie, pgrep <exe>)
#   # exe = "nginx"
#   ## pattern as argument for pgrep (ie, pgrep -f <pattern>)
#   # pattern = "nginx"
#   ## user as argument for pgrep (ie, pgrep -u <user>)
#   # user = "nginx"
#   ## Systemd unit name, supports globs when include_systemd_children is set to true
#   # systemd_unit = "nginx.service"
#   # include_systemd_children = false
#   ## CGroup name or path, supports globs
#   # cgroup = "systemd/system.slice/nginx.service"
#   ## Supervisor service names of hypervisorctl management
#   # supervisor_units = ["webserver", "proxy"]
#
#   ## Windows service name
#   # win_service = ""
#
#   ## override for process_name
#   ## This is optional; default is sourced from /proc/<pid>/status
#   # process_name = "bar"
#
#   ## Field name prefix
#   # prefix = ""
#
#   ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.
#   # mode = "irix"
#
#   ## Add the given information tag instead of a field
#   ## This allows to create unique metrics/series when collecting processes with
#   ## otherwise identical tags. However, please be careful as this can easily
#   ## result in a large number of series, especially with short-lived processes,
#   ## creating high cardinality at the output.
#   ## Available options are:
#   ##   cmdline   -- full commandline
#   ##   pid       -- ID of the process
#   ##   ppid      -- ID of the process' parent
#   ##   status    -- state of the process
#   ##   user      -- username owning the process
#   ## socket only options:
#   ##   protocol  -- protocol type of the process socket
#   ##   state     -- state of the process socket
#   ##   src       -- source address of the process socket (non-unix sockets)
#   ##   src_port  -- source port of the process socket (non-unix sockets)
#   ##   dest      -- destination address of the process socket (non-unix sockets)
#   ##   dest_port -- destination port of the process socket (non-unix sockets)
#   ##   name      -- name of the process socket (unix sockets only)
#   ## Available for procstat_lookup:
#   ##   level     -- level of the process filtering
#   # tag_with = []
#
#   ## Properties to collect
#   ## Available options are
#   ##   cpu     -- CPU usage statistics
#   ##   limits  -- set resource limits
#   ##   memory  -- memory usage statistics
#   ##   mmap    -- mapped memory usage statistics (caution: can cause high load)
#   ##   sockets -- socket statistics for protocols in 'socket_protocols'
#   # properties = ["cpu", "limits", "memory", "mmap"]
#
#   ## Protocol filter for the sockets property
#   ## Available options are
#   ##   all  -- all of the protocols below
#   ##   tcp4 -- TCP socket statistics for IPv4
#   ##   tcp6 -- TCP socket statistics for IPv6
#   ##   udp4 -- UDP socket statistics for IPv4
#   ##   udp6 -- UDP socket statistics for IPv6
#   ##   unix -- Unix socket statistics
#   # socket_protocols = ["all"]
#
#   ## Method to use when finding process IDs.  Can be one of 'pgrep', or
#   ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while
#   ## the native finder performs the search directly in a manor dependent on the
#   ## platform.  Default is 'pgrep'
#   # pid_finder = "pgrep"
#
#   ## New-style filtering configuration (multiple filter sections are allowed)
#   # [[inputs.procstat.filter]]
#   #    ## Name of the filter added as 'filter' tag
#   #    name = "shell"
#   #
#   #    ## Service filters, only one is allowed
#   #    ## Systemd unit names (wildcards are supported)
#   #    # systemd_units = []
#   #    ## CGroup name or path (wildcards are supported)
#   #    # cgroups = []
#   #    ## Supervisor service names of hypervisorctl management
#   #    # supervisor_units = []
#   #    ## Windows service names
#   #    # win_service = []
#   #
#   #    ## Process filters, multiple are allowed
#   #    ## Regular expressions to use for matching against the full command
#   #    # patterns = ['.*']
#   #    ## List of users owning the process (wildcards are supported)
#   #    # users = ['*']
#   #    ## List of executable paths of the process (wildcards are supported)
#   #    # executables = ['*']
#   #    ## List of process names (wildcards are supported)
#   #    # process_names = ['*']
#   #    ## Recursion depth for determining children of the matched processes
#   #    ## A negative value means all children with infinite depth
#   #    # recursion_depth = 0

###############################################################################
#                            SERVICE INPUT PLUGINS                            #
###############################################################################
